---
title: "Team 4 - Heart Disease Detection"
author: "Claire Bentzen, John Vincent Deniega, Ravita Kartawinata"
date: "`r Sys.Date()`"
format: 
    #html: 
    #    toc: true
    pdf: default
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library}
suppressPackageStartupMessages(library(caret))
library(tidyr)
library(tidyverse)
library(gt)
library(dplyr)
library(tibble)
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(ggplot2))
library(corrplot)
library(ggplot2)
library(gridExtra)
```

```{r preprocess}
seed <- 123
#Ingest
data <- read.csv("heart.csv") #Change to your respective local path
```

```{r preprocess}
#Check for missing columns
missing_col <- colSums(is.na(data))
cat('No missing values found: \n')
missing_col

#Check for duplicate rows
duplicate_row <- data[duplicated(data),]
cat('Count of duplicate rows: ', nrow(duplicate_row),'\n')

data1 <- data[!duplicated(data),]
cat('NewData dimension: ',nrow(data1),'remaining rows. This is still sufficient since ncol^2 is less than #nrows\n')

# Clip outliers to 1.5 times greater than minimum Q1 or maximum Q3 quartile prior to center and scaling so as not to excessively affect mean and standard deviation
cont_pred <- c('age', 'chol', 'oldpeak', 'thalach', 'trestbps')
cont_data_toclip <- data1[, cont_pred]
max(cont_data_toclip$chol) # test chol max = 564
clip_outlier <- function(x){
      q1 <- quantile(x, .25, na.rm = TRUE)
      q3 <- quantile(x, .75, na.rm = TRUE)
      IQR <- q3 - q1
      lower <- q1 - 1.5 * IQR
      upper <- q3 + 1.5 * IQR
      x <- ifelse(x < lower, lower, x)
      x <- ifelse(x > upper, upper, x)
      return(x)
}
cont_data_clipped <- cont_data_toclip |> mutate(across(everything(), clip_outlier))
max(cont_data_clipped) # test chol max = 370.375: Success. Outliers mitigated.

summary(data1)

data1$age <- cont_data_clipped$age
data1$trestbps <-cont_data_clipped$trestbps
data1$chol <- cont_data_clipped$chol
data1$thalach <- cont_data_clipped$thalach
data1$oldpeak <- cont_data_clipped$oldpeak
summary(data1)

#Center and scale continuous variables
pre_proc <- preProcess(data1[c("age", "trestbps", "chol", "thalach", "oldpeak")], method = c("center", "scale"))
data2 <- data1
data2[c("age", "trestbps", "chol", "thalach", "oldpeak")] <- 
  predict(pre_proc, data1[c("age", "trestbps", "chol", "thalach", "oldpeak")])

#Since "thal" is not ordinal, but effectively categorical, make dummy variables
data3 <- data2
data3$thal <- factor(data3$thal)
dummy <- dummyVars(~ thal, data = data3)
dummy_col <- predict(dummy, newdata = data3)
data3 <- cbind(data3, dummy_col)
data3$thal <- NULL # Drop the "thal" column now that we have dummy variables
cat('NewData dimension: ',nrow(data3),'remaining rows. This is still sufficient since ncol^2 is less than #nrows \n')
```

```{r EDA}
#Check column histograms for unusual distributions
ggplot(gather(data3), aes(value)) +
  geom_histogram(bins = 20) +
  facet_wrap(~key, scales = "free")
```

```{r EDA_orig_var}
cont_pred <- c('age', 'chol', 'oldpeak', 'thalach', 'trestbps')
cont_data <- data1[, cont_pred]
cont_data_long <- gather(cont_data)

# boxplots
ggplot(cont_data_long, aes(x = key, y = value, fill = key)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 1) +
  stat_summary(fun = mean, geom = "point", shape = 20, size = 3, color = "blue", fill = "blue") +
  labs(title = "Boxplots for Continuous Predictors of Heart Disease",
       x = "Predictor",
       y = "Value (raw)",
       caption = "Note: Blue dots display mean values") +
  theme_minimal(base_size = 15) +
  theme(plot.title = element_text(hjust = 0.5, size = 15, face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank(),
        legend.position = "none") +
  scale_fill_brewer(palette = "Set3") +
  geom_hline(yintercept = 0, color = "red", size = .5, linetype = "dashed")
```

```{r EDA2_processed}
# continuous predictors
cont_pred <- c('age', 'chol', 'oldpeak', 'thalach', 'trestbps')
cont_data <- data3[, cont_pred]
cont_data_long <- gather(cont_data) # 
ex_cont_data <- data3[,!names(data3) %in% cont_pred]
# boxplots
ggplot(cont_data_long, aes(x = key, y = value, fill = key)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 1) +
  stat_summary(fun = mean, geom = "point", shape = 20, size = 3, color = "blue", fill = "blue") +
  labs(title = "Boxplots for Continuous Predictors of Heart Disease",
       x = "Predictor",
       y = "Value (centered and scaled)",
       caption = "Note: Blue dots display mean values") +
  theme_minimal(base_size = 15) +
  theme(plot.title = element_text(hjust = 0.5, size = 15, face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank(),
        legend.position = "none") +
  scale_fill_brewer(palette = "Set3") +
  geom_hline(yintercept = 0, color = "red", size = .5, linetype = "dashed")
```

```{r EDA3}
data3$target <- as.factor(data3$target)
data3$sex <- as.factor(data3$sex)
# stacked bar plot for heart disease detection in males vs females

ggplot(data3, aes(x = sex, fill = target)) +
  geom_bar() +
  labs(title = "Bar Plot of Sex by Heart Disease Detection",
       x = "Sex",
       y = "Count",
       fill = "Target") +
  scale_x_discrete(labels = c("0" = "Female", "1" = "Male")) +
  theme_minimal()
```

```{r EDA4}
data3$target <- as.factor(data3$target)
data3$thal.1 <- as.factor(data3$thal.1)
data3$thal.2 <- as.factor(data3$thal.2)
data3$thal.3 <- as.factor(data3$thal.3)

# stacked bar plot for heart disease detection in thal.1
ggplot(data3, aes(x = thal.1, fill = target)) +
  geom_bar() +
  labs(title = "Bar Plot of Thal = Normal by Heart Disease Detection",
       x = "Thal = Normal",
       y = "Count",
       fill = "Heart Disease") +
  scale_x_discrete(labels = c("0" = "Other", "1" = "Normal")) +
  theme_minimal()

# stacked bar plot for heart disease detection in thal.2
ggplot(data3, aes(x = thal.2, fill = target)) +
  geom_bar() +
  labs(title = "Bar Plot of Thal = Fixed Defect by Heart Disease Detection",
       x = "Thal = Fixed Defect",
       y = "Count",
       fill = "Heart Disease") +
  scale_x_discrete(labels = c("0" = "Other", "1" = "Fixed Defect")) +
  theme_minimal()

# stacked bar plot for heart disease detection in thal.3
ggplot(data3, aes(x = thal.3, fill = target)) +
  geom_bar() +
  labs(title = "Bar Plot of Thal = Reversible Defect by Heart Disease Detection",
       x = "Thal = Reversable Defect",
       y = "Count",
       fill = "Heart Disease") +
  scale_x_discrete(labels = c("0" = "Other", "1" = "Reversible Defect")) +
  theme_minimal()
```

```{r EDA5}
# histogram of age segmented by heart disease detection
ggplot(data3, aes(x = age, fill = target)) +
  geom_histogram(binwidth = .1, position = "stack", alpha = 0.5) +
  labs(title = "Histogram Segmented by Heart Disease Detection",
       x = "Age",
       y = "Frequency",
       fill = "Heart Disease") +
  theme_minimal()
```

```{r preprocess}
#Check for near-zero variance columns
nzv <- nearZeroVar(data3)
cat('Removed near zero predictor: ', colnames(data3)[nzv],'\n')

data4 <- data3[, -nearZeroVar(data3)]
cat('NewData dimension: ',nrow(data4),'rows', ncol(data4), 'columns\n') 
# Note: There appears to be an error in documentation where thal is actually thal+1 category
# Normal = 1
# Fixed Defect = 2
# Reversable Defect = 3
```

```{r correlation}
# remove highly correlated predictors
p_correlations <- cor(cont_data, method = "pearson")
p_highCorr <- findCorrelation(p_correlations, cutoff = .75)
p_highCorr # No continuous variables correlated > .75

ex_cont_data$sex <- as.integer(ex_cont_data$sex)
ex_cont_data$thal.1 <- as.integer(ex_cont_data$thal.1)
ex_cont_data$thal.2 <- as.integer(ex_cont_data$thal.2)
ex_cont_data$thal.3 <- as.integer(ex_cont_data$thal.3)

s_correlations <- round(cor(ex_cont_data[, !names(ex_cont_data) %in% "target"], 
                      method = "spearman"), 3)
s_highCorr <- findCorrelation(s_correlations, cutoff = .75)
data5 <- data4[, -s_highCorr]
cont_data
# correlation plot among predictor variables
p_correlations <- cor(cont_data)
c_correlations <- cor(ex_cont_data)
p_corr_plot <- corrplot(p_correlations)
c_corr_plot <- corrplot(c_correlations)
```

```{r confounders}
# logistic regression model with possible confounding predictors
confounding_model <- glm(target ~ age + sex + thal.2 + thal.3, data = data5, family = "binomial")
summary(confounding_model)
```

The low p-values for age, sex, and thal.2 indicate that these are possible confounding variables, however it does not confirm it. We have to now look at the correlation between the predictors and the target variable.

```{r conf_corr}
# correlation between possible confounders and target
data_conf_check <- data5

data_conf_check$target <- as.integer(data_conf_check$target)
data_conf_check$age <- as.integer(data_conf_check$age)
data_conf_check$sex <- as.integer(data_conf_check$sex)
data_conf_check$thal.2 <- as.integer(data_conf_check$thal.2)
data_conf_check
cor(data_conf_check[c("target", "age", "sex", "thal.2")])
```

Age and sex have a very low correlation with the target variable, so we can keep them. The correlation thal.2 has with the target variable is moderate, but not high enough to indicate that there might be significant changes to the outcome of the model if we keep it.

```{r split}
set.seed(seed)
y <- data5$target 
x <- data5[, !names(data5) %in% "target"] # Predictors only

trainingRows <- createDataPartition(y, p = .80, list = FALSE) #list of indices from training
train_y <- y[trainingRows]
test_y <- y[-trainingRows]
train_x <- x[trainingRows, ] 
test_x <- x[-trainingRows, ]
train_x <- data.frame(lapply(train_x, function(x) if(is.factor(x)) as.numeric(as.character(x)) else x))
cat('Number of training sample:', nrow(train_x), 'and test samples: ',nrow(test_x), 'number of predictors:', ncol(train_x))
```

```{r cross validation and hypertune binary classify}
# Evaluation Metric
sens_spec_harm <- function(data, lev = NULL, model = NULL) {
  sens <- sensitivity(data$pred, data$obs, positive = levels(data$obs)[1])
  spec <- specificity(data$pred, data$obs, positive = levels(data$obs)[1])
  harmonic <- (2 * sens * spec) / (sens + spec)
  suppressMessages({
  roc <- roc(response = data$obs, 
             predictor = as.numeric(data$pred), 
             levels = rev(levels(data$obs)))
  })
  auc <- auc(roc)
  
  c(harmonic = harmonic, 
    sensitivity = sens, 
    specificity = spec,
    auc = as.numeric(auc))
}

ctrl <- trainControl(method = "repeatedcv", repeats = 5,
                     summaryFunction = sens_spec_harm,
                     classProbs = TRUE,
                     savePredictions = TRUE)

tunegrid <- expand.grid(alpha = c(0,  .1,  .2, .4, .6, .8, 1),
                        lambda = seq(.01, .2, length = 10))

```

```{r model echo=FALSE}
levels(train_y) <- make.names(levels(train_y))
# Logistic Regression
LR_model <-suppressWarnings(
  train(x = train_x, y = train_y,
           method = "glm",
           #tuneGrid = tunegrid,
           preProc = c("center", "scale"),
           metric = "sens_spec_harm",
           trControl = ctrl))

#linear discriminant
set.seed(476)
LDA_model <-suppressWarnings(
  train(x = train_x, y = train_y,
            method = "lda",
            preProc = c("center", "scale"),
            metric = "sens_spec_harm",
            trControl = ctrl))
#penalized logistic regression
set.seed(476)
PLR_model <-  suppressWarnings(
  train(x = train_x, y = train_y,
           method = "glmnet",
           tuneGrid = tunegrid,
           preProc = c("center", "scale"),
           metric = "sens_spec_harm",
           trControl = ctrl))
#nearest shrunken centroids
set.seed(476)
tunegrid <- expand.grid(threshold = seq(0, 25, length = 30))
NSC_model <-suppressWarnings(
  train(x = train_x, y = train_y,
          method = "pam",
          preProc = c("center", "scale"),
          tuneGrid = tunegrid,
          metric = "sens_spec_harm",
          trControl = ctrl))
```

```{r model echo=FALSE}

nnetGrid <- expand.grid(decay = c(0, 0.01, .1), 
                        size = c(3, 7, 11, 13))

# Neural Network
set.seed(123)
nn_model <- suppressWarnings(train(x = train_x, y = train_y,
    method = "nnet",
    tuneGrid = nnetGrid,
    trControl = ctrl,
    preProc = c("center", "scale"),
    metric = "sens_spec_harm",
    linout = FALSE,
    trace = FALSE))

# Support Vector Machine 
set.seed(123)
svm_model <-  train(x = train_x, y = train_y,
                  method = "svmRadial",
                  preProc = c("center", "scale"),
                  metric = "sens_spec_harm",
                  tuneLength = 14,
                  trControl = ctrl)
# k-Nearest Neighbors
set.seed(123)
knn_model <-  train(x = train_x, y = train_y,
                 method = "knn",
                 preProc = c("center", "scale"),
                 metric = "sens_spec_harm",
                 tuneGrid = data.frame(k = 1:20),
                 trControl = ctrl)
```

```{r model-performance}
#Harmonic
LR_sens_spec <- LR_model$results$harmonic
LDA_sens_spec <- LDA_model$results$harmonic
PLR_sens_spec <- mean(PLR_model$results$harmonic)
NSC_sens_spec <- mean(NSC_model$results$harmonic)

# Using values of models optimized for "harmonic"
NN_sens_spec <- max(nn_model$results$harmonic) # use size = 13 and decay = .1
SVM_sens_spec <- max(svm_model$results$harmonic) # use S=.05041146 andC=.25
KNN_sens_spec <- max(knn_model$results$harmonic) # use k=19

sens_spec_values <- data.frame(
  Model = c("LogisticRegression", 
            "LinearDiscriminant",
            "PenalizedLogisticRegression",
            "NearestShrunkenCentroid",
            "Neural Net",
            "Support Vector Machine",
            "KNN"),
  F_Score_Sens_Spec = c(LR_sens_spec, 
                        LDA_sens_spec, 
                        PLR_sens_spec,
                        NSC_sens_spec,
                        NN_sens_spec,
                        SVM_sens_spec,
                        KNN_sens_spec)
)

#confusion Matrix
LR_CM <- confusionMatrix(LR_model, norm="none")
LDA_CM <- confusionMatrix(LDA_model, norm="none")
PLR_CM <- confusionMatrix(PLR_model, norm="none")
NSC_CM <- confusionMatrix(NSC_model, norm="none")
NN_CM <- confusionMatrix(nn_model, norm="none")
SVM_CM <- confusionMatrix(svm_model, norm="none")
KNN_CM <- confusionMatrix(knn_model, norm="none")

#ROC-AUC  --> added auc field
LR_auc <- LR_model$results$auc
LDA_auc <- LDA_model$results$auc
PLR_auc <- mean(PLR_model$results$auc)
NSC_auc <- mean(NSC_model$results$auc)
NN_auc <- mean(nn_model$results$auc)
SVM_auc <- mean(svm_model$results$auc)
KNN_auc <- mean(knn_model$results$auc)


Model_performance <-  data.frame(
  Model = c("LogisticRegression", 
            "LinearDiscriminant",
            "PenalizedLogisticRegression",
            "NearestShrunkenCentroid",
            "Neural Net",
            "Support Vector Machine",
            "KNN"),
  Accuracy = c(
      sum(diag(LR_CM$table))/ sum(LR_CM$table),
      sum(diag(LDA_CM$table))/ sum(LDA_CM$table),
      sum(diag(PLR_CM$table))/ sum(PLR_CM$table),
      sum(diag(NSC_CM$table))/ sum(NSC_CM$table),
      sum(diag(NN_CM$table))/ sum(NN_CM$table),
      sum(diag(SVM_CM$table))/ sum(SVM_CM$table),
      sum(diag(KNN_CM$table))/ sum(KNN_CM$table)
    ),
  AUC = c(LR_auc, LDA_auc, PLR_auc, NSC_auc, NN_auc, SVM_auc, KNN_auc),
  Harmonic = c(LR_sens_spec, 
               LDA_sens_spec, 
               PLR_sens_spec, 
               NSC_sens_spec,
               NN_sens_spec,
               SVM_sens_spec,
               KNN_sens_spec)
)
#Sort by optimal detection of all disease cases + minimizing false negatives
Model_performance |> arrange(desc(Harmonic))
```

```{r validation_CI}
# resample validation results
model_metrics <- resamples(list(
  LDA = LDA_model,
  PLR = PLR_model,
  SVM = svm_model,
  KNN = knn_model,
  LR = LR_model,
  NSC = NSC_model,
  NN = nn_model
))

# plot harmonic mean confidence intervals
dotplot(model_metrics, metric = "harmonic", main = "Harmonic Mean Resampling Model Performance")

# plot AUC confidence intervals
dotplot(model_metrics, metric = "auc", main = "AUC Resampling Model Performance")
```

```{r predict}
# convert all predictors to numeric
test_x <- data.frame(lapply(test_x, function(x) if (is.factor(x)) as.numeric(as.character(x)) else x))

levels(test_y) <- make.names(levels(test_y))
              
# predict test data with each model        
test_results <- data.frame(obs = test_y, 
                           LR = predict(LR_model, test_x))

test_results$LDA <- predict(LDA_model, test_x)

test_results$PLR <- predict(PLR_model, test_x)

test_results$NSC <- predict(NSC_model, test_x)

test_results$nnet <- predict(nn_model, test_x)
test_results$svm <- predict(svm_model, test_x)
test_results$knn <- predict(knn_model, test_x)
```

```{r comparison}
# model comparison using confusion matrix
LR_CM_pred <- confusionMatrix(test_results$LR, test_results$obs, positive = "X1")
LDA_CM_pred <- confusionMatrix(test_results$LDA, test_results$obs, positive = "X1")
PLR_CM_pred <- confusionMatrix(test_results$PLR, test_results$obs, positive = "X1")
NSC_CM_pred <- confusionMatrix(test_results$NSC, test_results$obs, positive = "X1")
nnet_CM_pred <- confusionMatrix(test_results$nnet, test_results$obs, positive = "X1")
svm_CM_pred <- confusionMatrix(test_results$svm, test_results$obs, positive = "X1")
knn_CM_pred <- confusionMatrix(test_results$knn, test_results$obs, positive = "X1")

#sens and spec are temporary variables to be overwritten with each model
#for the purpose of saving result in their own permanent variable
sens <- LR_CM_pred$byClass['Sensitivity']
spec <- LR_CM_pred$byClass['Specificity']
LR_harm_pred <- (2 * sens * spec) / (sens + spec)
sens <- LDA_CM_pred$byClass['Sensitivity']
spec <- LDA_CM_pred$byClass['Specificity']
LDA_harm_pred <- (2 * sens * spec) / (sens + spec)
sens <- PLR_CM_pred$byClass['Sensitivity']
spec <- PLR_CM_pred$byClass['Specificity']
PLR_harm_pred <- (2 * sens * spec) / (sens + spec)
sens <- NSC_CM_pred$byClass['Sensitivity']
spec <- NSC_CM_pred$byClass['Specificity']
NSC_harm_pred <- (2 * sens * spec) / (sens + spec)
sens <- nnet_CM_pred$byClass['Sensitivity']
spec <- nnet_CM_pred$byClass['Specificity']
nnet_harm_pred <- (2 * sens * spec) / (sens + spec)
sens <- svm_CM_pred$byClass['Sensitivity']
spec <- svm_CM_pred$byClass['Specificity']
svm_harm_pred <- (2 * sens * spec) / (sens + spec)
sens <- knn_CM_pred$byClass['Sensitivity']
spec <- knn_CM_pred$byClass['Specificity']
knn_harm_pred <- (2 * sens * spec) / (sens + spec)

#ROC
LR_roc_pred <- roc(test_results$obs, as.numeric(test_results$LR))
LDA_roc_pred <- roc(test_results$obs, as.numeric(test_results$LDA))
PLR_roc_pred <- roc(test_results$obs, as.numeric(test_results$PLR))
NSC_roc_pred <- roc(test_results$obs, as.numeric(test_results$NSC))
nnet_roc_pred <- roc(test_results$obs, as.numeric(test_results$nnet))
svm_roc_pred <- roc(test_results$obs, as.numeric(test_results$svm))
knn_roc_pred <- roc(test_results$obs, as.numeric(test_results$knn))

Test_performance <-  data.frame(
  Model = c("LogisticRegression", "LinearDiscriminant",
  "PenalizedLogisticRegression", "NearestShrunken",
  "NeutralNetwork","SVM","kNN"),
  Accuracy = c(
    LR_CM_pred$overall['Accuracy'],
    LDA_CM_pred$overall['Accuracy'],
    PLR_CM_pred$overall['Accuracy'], 
    NSC_CM_pred$overall['Accuracy'],
    nnet_CM_pred$overall['Accuracy'], 
    svm_CM_pred$overall['Accuracy'], 
    knn_CM_pred$overall['Accuracy']
    ),
  AUC = c(
    LR_roc_pred$auc,
    LDA_roc_pred$auc,
    PLR_roc_pred$auc,
    NSC_roc_pred$auc,
    nnet_roc_pred$auc,
    svm_roc_pred$auc,
    knn_roc_pred$auc
  ),
  Harmonic = c(LR_harm_pred, LDA_harm_pred, PLR_harm_pred, NSC_harm_pred,
               nnet_harm_pred, svm_harm_pred, knn_harm_pred)
)
Test_performance |> arrange(desc(Harmonic))
```

```{r roc_plot}
# compare ROC curves
plot(LDA_roc_pred, col = "red", main = "ROC Curves")
lines(PLR_roc_pred, col = "blue")
lines(svm_roc_pred, col = "green")
lines(knn_roc_pred, col = "pink")
lines(LR_roc_pred, col = "purple")
lines(NSC_roc_pred, col = "orange")
lines(nnet_roc_pred, col = "yellow")
legend("bottomright", legend = c("LDA", "PLR", "SVM", "KNN", "LR", "NSC", "NN"), col = c("red", "blue", "green", "pink", "purple", "orange", "yellow"), lwd = 2)
```

```{r test_CI}
# calculate 95% confidence intervals for harmonic test results
test_harmonic <- Test_performance[, c('Model', 'Harmonic')] %>%
  mutate(
    LowerCI = Harmonic - qt(0.975, df = nrow(test_x) - 1) * (sd(Harmonic) / sqrt(nrow(test_x))),
    UpperCI = Harmonic + qt(0.975, df = nrow(test_x) - 1) * (sd(Harmonic) / sqrt(nrow(test_x)))
  )

# plot harmonic confidence intervals
ggplot(test_harmonic, aes(x = Harmonic, y = Model)) +
  geom_point() +
  geom_errorbar(aes(xmin = LowerCI, xmax = UpperCI), width = 0.2) +
  labs(title = "Harmonic Mean - Model Performance on Test Data",
       x = "Harmonic Mean",
       y = "Model") +
  theme_minimal()

# calculate 95% confidence intervals for auc test results
test_auc <- Test_performance[, c('Model', 'AUC')] %>%
  mutate(
    LowerCI = AUC - qt(0.975, df = nrow(test_x) - 1) * (sd(AUC) / sqrt(nrow(test_x))),
    UpperCI = AUC + qt(0.975, df = nrow(test_x) - 1) * (sd(AUC) / sqrt(nrow(test_x)))
  )

# plot auc confidence intervals
ggplot(test_auc, aes(x = AUC, y = Model)) +
  geom_point() +
  geom_errorbar(aes(xmin = LowerCI, xmax = UpperCI), width = 0.2) +
  labs(title = "AUC - Model Performance on Test Data",
       x = "AUC",
       y = "Model") +
  theme_minimal()

# calculate 95% confidence intervals for accuracy test results
test_accuracy <- Test_performance[, c('Model', 'Accuracy')] %>%
  mutate(
    LowerCI = Accuracy - qt(0.975, df = nrow(test_x) - 1) * (sd(Accuracy) / sqrt(nrow(test_x))),
    UpperCI = Accuracy + qt(0.975, df = nrow(test_x) - 1) * (sd(Accuracy) / sqrt(nrow(test_x)))
  )

# plot auc confidence intervals
ggplot(test_accuracy, aes(x = Accuracy, y = Model)) +
  geom_point() +
  geom_errorbar(aes(xmin = LowerCI, xmax = UpperCI), width = 0.2) +
  labs(title = "Accuracy - Model Performance on Test Data",
       x = "Accuracy",
       y = "Model") +
  theme_minimal()
```

```{r Important_var_plot}
plots <- list()
top_vars <- list()

model_names <- c("LR_model", "LDA_model", "PLR_model", "NSC_model", "nn_model", "svm_model", "knn_model")
titles <- c("Logistic Regression", "LDA", "Penalized Logistic Regression", "NSC", "Neural Network", "SVM", "k-NN")

for (i in seq_along(model_names)) {
    model <- get(model_names[i])
    title <- titles[i]
    
    imp_var <- varImp(model, scale = FALSE)
    imp_var_df <- as.data.frame(imp_var$importance)
    imp_var_df$Variable <- rownames(imp_var$importance)
    
    # Check if the 'Overall' column exists
    if (!("Overall" %in% colnames(imp_var_df))) {
        imp_var_df <- imp_var_df %>%
            rowwise() %>%
            mutate(Overall = mean(c_across(starts_with("X")), na.rm = TRUE)) %>%
            ungroup()
    }
    
    top5_imp_var <- imp_var_df %>% arrange(desc(Overall)) %>% slice(1:5)
    
    p <- ggplot(top5_imp_var, aes(x = reorder(Variable, Overall), y = Overall)) +
        geom_bar(stat = "identity") +
        coord_flip() +
        ggtitle(title) +
        theme_minimal() +
        labs(x = "Variable", y = "Importance")
    
    plots[[i]] <- p
    top_vars[[i]] <- top5_imp_var$Variable

}
all_top_vars <- unlist(top_vars)
most_common_vars <- names(head(sort(table(all_top_vars), decreasing = TRUE), 5))
most_common_vars

do.call(grid.arrange, c(plots, ncol = 2))
```

```{r chosen_model}
# plot LDA variable importance
plot(varImp(LDA_model, scale = FALSE), main = "LDA Variable Importance")

# LDA Test Performance
Test_performance[2, ]
```
